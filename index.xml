<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>blog</title>
<link>https://infinitelylearn.github.io/blog/</link>
<atom:link href="https://infinitelylearn.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Ininitely learn</description>
<image>
<url>https://infinitelylearn.github.io/blog/images/tris-card.jpg</url>
<title>blog</title>
<link>https://infinitelylearn.github.io/blog/</link>
</image>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Thu, 26 Sep 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>20 Days of Exploration: What I’ve Been Up To</title>
  <dc:creator>Tris </dc:creator>
  <link>https://infinitelylearn.github.io/blog/posts/2024-09-26-intro-to-algorithmic-trading/</link>
  <description><![CDATA[ 




<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="what-have-i-been-up-to" class="level3">
<h3 class="anchored" data-anchor-id="what-have-i-been-up-to">What Have I Been Up To?</h3>
<p>Since my first post, I have to admit—things didn’t go exactly as planned. Initially, I intended to steadily work through tutorials, books, and courses, documenting every step of the way here on this blog. Well, I kept up with the learning, but the documenting… not so much!</p>
</section>
<section id="reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="reinforcement-learning">Reinforcement Learning</h2>
<p>My primary goal was to finish the first three chapters of Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto. For those who aren’t familiar, it’s a foundational book in the field—a go-to resource for understanding reinforcement learning (RL) concepts. So, I started there, and it has been eye-opening.</p>
<section id="chapter-1-an-overview-of-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="chapter-1-an-overview-of-reinforcement-learning">Chapter 1: An Overview of Reinforcement Learning</h3>
<p>The first chapter sets the foundation by introducing what RL is and where it fits in the larger context of machine learning. It covers key terminology and ideas, such as:</p>
<ul>
<li>The Concept of Agents: At its core, RL is about agents taking actions in an environment to maximise some notion of cumulative reward.</li>
<li>Value and Reward Functions: These are crucial concepts in RL. Value functions help an agent determine the “value” of a state (or state-action pair), while reward functions represent the feedback from the environment based on the agent’s actions.</li>
<li>History and Context: The chapter also touches upon the history of RL, tying it back to the roots of AI, psychology, and neuroscience, showing how RL is not just a computational concept but inspired by how living beings learn to interact with the world.</li>
</ul>
</section>
<section id="chapter-2-multi-armed-bandits" class="level3">
<h3 class="anchored" data-anchor-id="chapter-2-multi-armed-bandits">Chapter 2: Multi-Armed Bandits</h3>
<p>This chapter dives into one of the simplest forms of reinforcement learning problems—the multi-armed bandit. If you’ve never heard of a multi-armed bandit, imagine walking into a casino with slot machines (or “bandits”), each with different probabilities of paying out. Your goal is to figure out which machine to play to maximise returns, balancing exploration (trying out different machines) with exploitation (sticking to the best-performing machine). Techniques covered include:</p>
<ul>
<li>Epsilon-Greedy Strategy: A straightforward approach that allows for both exploitation of known rewards and exploration of potentially better options. With a small probability, you pick a random option; otherwise, you go with the current best.</li>
<li>Upper Confidence Bound (UCB): A more sophisticated method that considers uncertainty in estimates and prioritises options that could yield higher rewards while balancing exploration and exploitation.</li>
<li>Gradient Bandit Algorithms: Instead of estimating action values, this approach focuses on learning preferences over actions and updating them in a way that encourages the agent to try higher-reward options more frequently.</li>
</ul>
</section>
<section id="chapter-3-markov-decision-processes-mdps" class="level3">
<h3 class="anchored" data-anchor-id="chapter-3-markov-decision-processes-mdps">Chapter 3: Markov Decision Processes (MDPs)</h3>
<p>Chapter 3 builds on foundational concepts to introduce Markov Decision Processes (MDPs), which form the backbone of most RL problems. An MDP provides a mathematical framework for modelling decision-making in situations where outcomes are partly random and partly under the agent’s control. Key ideas include:</p>
<ul>
<li>The Agent-Environment Interface: The MDP framework views learning as a loop where an agent takes actions that influence its state within an environment, and the environment provides feedback via rewards.</li>
<li>Goals and Rewards: The chapter emphasises the importance of designing appropriate reward signals. A reward isn’t just about getting the agent to learn to achieve a goal; it’s also about ensuring it learns to achieve it in the “right” way. Proper reward design plays a significant role in guiding the agent’s learning and behaviour.</li>
</ul>
</section>
</section>
<section id="diving-into-algorithmic-trading" class="level2">
<h2 class="anchored" data-anchor-id="diving-into-algorithmic-trading">Diving into Algorithmic Trading</h2>
<p>After reading through the first three chapters above, I found myself at a crossroads—deciding what to study next. Reinforcement learning was (and still is) fascinating, but I wanted to apply some of the concepts more practically. One of my main goals is to build an algorithmic trading system, and I realised that focusing purely on theory before diving in wasn’t the most effective approach. There’s a saying that you learn best by doing, and I wanted to embrace that.</p>
<p>Rather than waiting to learn everything I “needed” before getting started, I decided to just jump in, get my hands dirty, and start building. The theoretical learning will always be there, but there’s nothing like a hands-on project to test, apply, and deepen your understanding.</p>
</section>
<section id="building-my-own-algorithmic-trading-system-a-journey-so-far" class="level2">
<h2 class="anchored" data-anchor-id="building-my-own-algorithmic-trading-system-a-journey-so-far">Building My Own Algorithmic Trading System: A Journey So Far</h2>
<p>I had previously built a basic framework for a trading system, so I decided to start improving on that. It wasn’t the most polished code, but it was a good place to start. The system essentially:</p>
<ul>
<li>Loaded 1-Minute Stock Price Data into Polars: Polars is a data manipulation library similar to Pandas but written in Rust, which makes it extremely fast for handling large datasets.</li>
<li>Transformed the Data: Within Polars, I manipulated stock data to create signals for a trading strategy—adding indicators, defining conditions for entry and exit, and so on.</li>
<li>Exported Data to Numpy Arrays: To efficiently simulate trades, I exported data to Numpy arrays and looped through each row, as if the trades were happening live.</li>
<li>Output and Visualisation: Finally, I collected trades and outcomes in a DataFrame for analysis and visualisation.</li>
</ul>
<section id="why-polars" class="level4">
<h4 class="anchored" data-anchor-id="why-polars">Why Polars?</h4>
<p>The choice to use Polars was strategic. One of its standout features is lazy evaluation, which allows it to optimise processing. For example, say you load 20 years of stock price data, manipulate it, and only need the last year for analysis. Polars will evaluate these operations at the last possible moment, potentially avoiding the need to load the first 19 years into memory. It also optimises operations across multiple threads, offering significant performance boosts when working with large datasets—essential for backtesting trading strategies over extensive periods.</p>
</section>
</section>
<section id="the-learning-curve-insights-and-challenges" class="level2">
<h2 class="anchored" data-anchor-id="the-learning-curve-insights-and-challenges">The Learning Curve: Insights and Challenges</h2>
<section id="refactoring-for-modularity" class="level3">
<h3 class="anchored" data-anchor-id="refactoring-for-modularity">Refactoring for Modularity</h3>
<p>The first step was to clean up the code for better organisation and modularity. Initially, much of the backtesting logic was intertwined with strategy-specific details, so I aimed to decouple these. Now, each strategy is a contained function, which can be passed as an argument to a generalised backtesting function. This separation allowed me to:</p>
<ul>
<li>Isolate Strategy Implementation: Define the indicators, entry and exit rules, take profit (TP), and stop loss (SL) within each strategy.</li>
<li>Generalise Backtesting Logic: Simulate trades based on any strategy provided, improving code reusability and ensuring consistency for both backtesting and future live trading.</li>
</ul>
</section>
<section id="backtesting-with-1-minute-data" class="level3">
<h3 class="anchored" data-anchor-id="backtesting-with-1-minute-data">Backtesting with 1-Minute Data</h3>
<p>After restructuring the code, I backtested a simple strategy on the last 10 years of 1-minute data from the NDX. Initially, each backtest took around 30 seconds to complete—a significant delay when running multiple tests. My goal was to reduce this to as close to 1 second as possible.</p>
<p>The backtesting process was:</p>
<ul>
<li>Load the Data: Import stock price data at 1-minute intervals.</li>
<li>Run Strategy through Backtest: Feed the strategy function into the backtesting function, along with the data and any relevant parameters.</li>
<li>Strategy Logic: Calculate indicators, such as a 30-period exponential moving average (EMA) on the 5-minute timeframe, and define entry/exit rules (e.g., entry signal if the close price is above EMA, exit if it drops below).</li>
<li>TP/SL Columns: Define take profit and stop loss levels (e.g., TP at 5% above close price, SL at 5% below), and output a DataFrame with the original data and added columns for indicators, signals, and TP/SL levels.</li>
</ul>
</section>
<section id="balancing-vectorisation-and-looping" class="level3">
<h3 class="anchored" data-anchor-id="balancing-vectorisation-and-looping">Balancing Vectorisation and Looping</h3>
<p>Initially, I tried a vectorised approach to find the next entry, exit, or TP/SL hit, isolating each trade’s range. If a TP/SL was hit before the exit signal, the trade would close; if not, it would close at the exit. This approach worked partially but struggled when accounting for dependent trade sequences. For example, a trade entered at 8am might exit at 9am due to hitting a stop loss, but another entry signal at 9:05am might trigger a re-entry.</p>
<p>Eventually, I adopted a hybrid approach: exporting all columns from Polars to Numpy arrays and iterating through each row to simulate trades sequentially. Using Numba—a high-performance Python compiler—I could speed up this process dramatically, reducing the backtesting time to just under 1 second.</p>
<ul>
<li>Strategy Manipulation &amp; Numpy Export: About 0.6 seconds.</li>
<li>Looping through Data in Numba: A mere 0.002 seconds per backtest.</li>
</ul>
<p>By carefully optimising data processing and the timing of Polars evaluations, I managed to achieve my goal, making it possible to iterate quickly on strategy ideas.</p>
</section>
</section>
<section id="looking-ahead-diving-deeper-into-quantitative-trading" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead-diving-deeper-into-quantitative-trading">Looking Ahead: Diving Deeper into Quantitative Trading</h2>
<p>While I was able to backtest a single strategy in under a second, I started wondering: what if I want to test a strategy on thousands of stocks at once? Simply repeating this process for 4,000 stocks could take over an hour to complete.</p>
<p>I began exploring whether I could load all the data into one massive DataFrame or break it into chunks for efficient processing. The first step was creating 4,000 stock data files. I took a single file with 20 years of 1-minute AAPL stock data (around 60MB as a Parquet file), randomly altered values, and wrote new files. However, I quickly realised I didn’t have enough local storage. Writing these files to an external USB3 SSD was significantly slower, raising questions about the feasibility of reading such large volumes of data quickly.</p>
<p>The scale—over 250GB—also brought into question how I’d handle the memory limitations of my machine (a modest 8GB of RAM). Around this time, I stumbled upon Python for Algorithmic Trading Cookbook, which discusses using ArcticDB, SQLite, and HDF5 for storing and accessing large datasets. This made me realise that my current approach of using Parquet files and Polars might not be scalable.</p>
<section id="exploring-new-storage-approaches" class="level3">
<h3 class="anchored" data-anchor-id="exploring-new-storage-approaches">Exploring New Storage Approaches</h3>
<p>My current focus is on researching these alternative storage solutions and their implications for my system. Before investing too heavily into one method, I want to explore various options to find the best fit for efficient backtesting and live trading. There’s a lot to learn, and I’m excited to dive in and share what I discover.</p>
</section>
<section id="embracing-the-learning-process" class="level3">
<h3 class="anchored" data-anchor-id="embracing-the-learning-process">Embracing the Learning Process</h3>
<p>The past 20 days have been a whirlwind of learning, coding, and adapting to new challenges. Though my journey has diverged from my initial plan, the hands-on experience has deepened my understanding of algorithmic trading, data manipulation, and backtesting optimisation. From exploring the inner workings of Polars to speeding up backtesting with Numba, I’ve discovered the importance of diving into practical challenges headfirst.</p>
<p>The next phase is to dive deeper into scalable data storage, refine backtesting across larger datasets, and ensure my approach is robust enough for live trading. I’m excited to share this evolving journey with you, and if you have any thoughts, questions, or tips based on your own experience, please reach out—I’d love to learn together!</p>
<p>Thank you for reading, you can find <a href="https://twitter.com/@InfinitelyLearn">me on X</a>, and stay tuned for more updates!</p>


</section>
</section>

 ]]></description>
  <category>explanation</category>
  <guid>https://infinitelylearn.github.io/blog/posts/2024-09-26-intro-to-algorithmic-trading/</guid>
  <pubDate>Thu, 26 Sep 2024 00:00:00 GMT</pubDate>
  <media:content url="https://infinitelylearn.github.io/blog/posts/2024-09-26-intro-to-algorithmic-trading/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>To Infinity and Beyond: My Deep Learning Journey</title>
  <dc:creator>Tris </dc:creator>
  <link>https://infinitelylearn.github.io/blog/posts/2024-09-05-my-journey-into-deep-learning-and-beyond/</link>
  <description><![CDATA[ 




<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>Hi, I’m Tris, and welcome to my blog - Infinitely Learn. My background is nothing to do with artificial intelligence, deep learning, or even computer or data science. At uni I studied a BSc in Audio &amp; Recording Technology.</p>
<p>One day in the studio we had a guest hardcore techno producer come in to do a workshop. In this class we’d been used to doing things like learning / experimenting with mic technique, acoustics, or recording local indie or jazz bands. This producer asked us to approach problems where we had to start thinking outside the box. Tasks like recreating the US national anthem, but in a hardcore techno style. Usually we aimed to record the cleanest, most detailed and transparent sound possible. For this task though I started to think of any possible way to distort, crush, and break the sound and mess it up as much as possible, aiming to recreate a hardcore kind of sound and texture according to the brief.</p>
<p>After the session, he asked me to stay behind and said, “Whatever you want to do, you can put your mind to it, and you can do it.” That’s stuck with me ever since. It’s something I remind myself of—if you’re determined enough, willing to adapt and apply yourself creatively, with enough time and practice, you can learn to do anything!</p>
<p>I think this applies to <strong>computational learning</strong> as well. Given enough time, training, and creativity in how you approach problems, anything is possible in the realm of artificial intelligence and machine learning.</p>
</section>
<section id="how-i-became-interested-in-artificial-intelligence" class="level2">
<h2 class="anchored" data-anchor-id="how-i-became-interested-in-artificial-intelligence">How I Became Interested in Artificial Intelligence</h2>
<p>A few years back, I got curious about fintech and started reading up on it. That’s where I first learned about AI and the massive impact it could have on the economy and society—what they call the Fourth Industrial Revolution. The more I read, the more excited I got about the potential of machine learning and deep learning.</p>
<p>During the COVID lockdowns, I had some time on my hands, so I started experimenting with creating various algorithmic trading strategies to backtest. I managed to backtest some strategies that did well for certain periods, but when I tested them over other periods, they completely failed. I realised that algorithmic trading isn’t just about building a perfect model; it’s about understanding the current market conditions and choosing the right algorithm, and adapting constantly. Plus, it made me aware of the whole challenge of overfitting and the importance of understanding how to avoid it while training machine learning models.</p>
<p>This has all been a passion project for me. I’m really excited about what I’ve been learning recently in the fields of deep learning, reinforcement learning, and quantitative finance, and what lies ahead. I’m keen to keep expanding my knowledge and getting more hands-on experience, and learn how to apply it to real problems.</p>
</section>
<section id="why-im-starting-this-blog" class="level2">
<h2 class="anchored" data-anchor-id="why-im-starting-this-blog">Why I’m Starting This Blog</h2>
<p>I realised the importance of recording and sharing what you learn to reinforce your understanding and accelerate learning, especially in areas like deep learning and machine learning that can become complex. I’m usually the type of person who loves to dive right into things without stopping to take proper notes. But many times, I have seen how much I learned—then forgot. “You don’t use it, you lose it!” This blog is in part my attempt to address that, helping me document my learning journey to refer back to and build upon.</p>
<p>Another reason for this blog is to share my learning with others. Maybe someone out there will find my experiences and insights in machine learning, deep learning, or quantitative finance useful or interesting. It also opens up opportunities for feedback, corrections, or discussions that can further enhance understanding, especially in a field as rapidly evolving as artificial intelligence.</p>
</section>
<section id="the-concept-to-infinitely-learn" class="level2">
<h2 class="anchored" data-anchor-id="the-concept-to-infinitely-learn">The Concept to Infinitely Learn</h2>
<p>The idea to “infinitely learn” came from several sources. One influence was fast.ai’s Jeremy Howard who said he spends a portion of every day learning something new, and the book Atomic Habits by James Clear, which emphasises the power of small, consistent actions and continuous learning over time. The main concept is that even if you don’t feel like you’re making significant progress day by day, you focus on the process not the goal and keep learning by habit and moving forward bit by bit, and the consistent small efforts add up and eventually lead to substantial growth in understanding complex topics like neural networks and data science.</p>
</section>
<section id="my-learning-experiences-so-far" class="level2">
<h2 class="anchored" data-anchor-id="my-learning-experiences-so-far">My Learning Experiences So Far</h2>
<p>Last year, I discovered the Harvard CS50x course and dived in. I watched all the lectures at 2x speed, did a few of the assignments, but didn’t take many notes. I did this with several other courses and books, including the Fast.ai course, which focuses on deep learning and neural networks. My goal was to cover as much ground as quickly as possible and grasp the overall concepts in computer science, machine learning, and deep learning. Now, to truly understand these concepts and apply them effectively, I’m trying to adopt a more thorough and reflective approach. That’s where this blog comes in—as a study tool to help me engage more deeply with the material, from basic algorithms to advanced reinforcement learning techniques.</p>
</section>
<section id="what-to-expect-from-this-blog" class="level2">
<h2 class="anchored" data-anchor-id="what-to-expect-from-this-blog">What to Expect from This Blog</h2>
<p>I’m expecting to explore several key areas in this blog, including:</p>
<ul>
<li>Deep Learning: Revisiting the Fast.ai course in greater depth, covering topics like neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).</li>
<li>Machine Learning: Delving into various algorithms, models, and practical applications to build a strong foundation in data science and AI.</li>
<li>Reinforcement Learning: Exploring this fascinating area of artificial intelligence, understanding key concepts, algorithms, and real-world applications in computational learning.</li>
<li>Quantitative Finance: More recently, I’ve taken an interest in this field. The world of ‘quants’ fascinates me, as it combines advanced mathematics, computer science, and finance. I’ll be sharing my journey into this world and how it intersects with AI and machine learning.</li>
</ul>
<p>The content will vary—sometimes I might post how-to guides or tutorials as I learn new processes or techniques in machine learning or reinforcement learning. Other times, I’ll reflect on personal insights and challenges I’ve faced in my AI learning journey. I don’t have a fixed plan for how this blog will evolve, and that’s part of what makes this journey into artificial intelligence and computational learning exciting. Sometimes, you just have to <strong>start</strong> without having an exact goal and see where it leads.</p>
</section>
<section id="join-me-on-this-journey" class="level2">
<h2 class="anchored" data-anchor-id="join-me-on-this-journey">Join Me on This Journey</h2>
<p>I’m excited to embark on this journey in AI, deep learning, quantitative finance, and beyond to a new level and share it with anyone who’s interested. If you’d like to follow along, you can find <a href="https://twitter.com/@InfinitelyLearn">me on X</a>. I’d love to hear from you and learn together about the fascinating world of machine learning, deep learning, and artificial intelligence.</p>
<p>Thank you for joining me on this journey.</p>
<section id="to-infinity-and-beyond" class="level3">
<h3 class="anchored" data-anchor-id="to-infinity-and-beyond">To infinity and beyond! ♾️</h3>


</section>
</section>

 ]]></description>
  <category>explanation</category>
  <guid>https://infinitelylearn.github.io/blog/posts/2024-09-05-my-journey-into-deep-learning-and-beyond/</guid>
  <pubDate>Thu, 05 Sep 2024 00:00:00 GMT</pubDate>
  <media:content url="https://infinitelylearn.github.io/blog/posts/2024-09-05-my-journey-into-deep-learning-and-beyond/cover.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
